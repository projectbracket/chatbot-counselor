{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "cJR2ah47PtHB",
        "outputId": "93604f7a-d097-4f65-a766-b983f9eb84ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.3.42-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 30 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 51 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 61 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Collecting ws4py\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▍                         | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 20 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 30 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 40 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 51 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51 kB 168 kB/s \n",
            "\u001b[?25hCollecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45229 sha256=d1ea748022c78ae5b45cc93f6f5e23dd407eda0e4d48f643668996d4d0f07266\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/ea/7d/3410aa0aa0e4402ead9a7a97ab2214804887e0f5c2b76f0c96\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.3.42 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install anvil-uplink\n",
        "from anvil import *\n",
        "import anvil.server\n",
        "anvil.server.connect(\"server_EMHCTR3AO64XZL2LTJR2LX4W-PU76NFNVYXV6QB4N\")\n",
        "\n",
        "from os import pathconf_names\n",
        "# libraries to deal with data types\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "\n",
        "# NLP tools that we'll need\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer # our stemmer\n",
        "nltk.download('punkt') # our tokenizer\n",
        "\n",
        "# neural network tools we'll need\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dropout, Dense\n",
        "\n",
        "# mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/Project Bracket Team 3/intents.json'\n",
        "\n",
        "with open(path, 'r') as f:\n",
        "  intents = json.load(f) \n",
        "\n",
        "\n",
        "# create our stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# creating our lists\n",
        "all_words = [] # our \"dictionary\"\n",
        "tags = [] # list of all our tags\n",
        "x = [] # list of all our patterns\n",
        "y = [] # associated tag for each pattern\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    # add our patterns and tags to x and y array\n",
        "    x.append(pattern)\n",
        "    y.append(intent['tag'])\n",
        "\n",
        "    # tokenize each pattern and add it to all words array\n",
        "    tokenizedPattern = nltk.word_tokenize(pattern)\n",
        "    all_words.extend(tokenizedPattern)\n",
        "\n",
        "    # add each tag to our general tags array\n",
        "    if intent[\"tag\"] not in tags:\n",
        "      tags.append(intent[\"tag\"])\n",
        "\n",
        "# stemming + lowering + remove punctuation from each word in all words array\n",
        "all_words = [stemmer.stem(words.lower()) for words in all_words if words not in string.punctuation]\n",
        "\n",
        "# organize alphabetically (for convenience) and remove duplicates\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "\n",
        "# let's train our chatbot\n",
        "train_data = []\n",
        "empty = [0] * len(tags)\n",
        "\n",
        "# creating the bag of words\n",
        "for index, pattern in enumerate(x):\n",
        "  bag = []\n",
        "  add_word = stemmer.stem(pattern.lower())\n",
        "  for word in all_words:\n",
        "    bag.append(1) if word in add_word else bag.append(0) # create our array of 1's and 0's\n",
        "\n",
        "# mark index of the tag to which pattern is associated to\n",
        "    marked_tag = list(empty)\n",
        "    marked_tag[tags.index(y[index])] = 1\n",
        "\n",
        "    train_data.append([bag, marked_tag])\n",
        "\n",
        "# shuffle  & turn into array\n",
        "random.shuffle(train_data)\n",
        "train_data_array = np.array(train_data, dtype=object)\n",
        "\n",
        "# split the features and target labels\n",
        "x = np.array(list(train_data_array[:, 0]))\n",
        "y = np.array(list(train_data_array[:, 1]))\n",
        "\n",
        "# feed our data into neural network\n",
        "\n",
        "# parameters\n",
        "input_nn = (len(x[0]),)\n",
        "output_nn = len(y[0])\n",
        "epochs = 200 # number of times algorithm will go through training data\n",
        "\n",
        "# defining the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=input_nn, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(output_nn, activation = \"softmax\"))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6) # adding required optimizer for model\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# print(model.summary())\n",
        "model.fit(x=x, y=y, epochs=200, verbose=0)\n",
        "\n",
        "def preprocess(msg): \n",
        "  tokenized_input = nltk.word_tokenize(msg)\n",
        "  tokenized_input = [stemmer.stem(w.lower()) for w in tokenized_input]\n",
        "  return tokenized_input\n",
        "\n",
        "def bagger(msg, all_words): \n",
        "  tokenized_input = preprocess(msg)\n",
        "  bag_of_words = [0] * len(all_words)\n",
        "  for wordSingle in tokenized_input: \n",
        "    for index, wordWhole in enumerate(all_words):\n",
        "      if wordWhole == wordSingle: \n",
        "        bag_of_words[index] = 1\n",
        "  return np.array(bag_of_words)\n",
        "\n",
        "def pred_class(msg, all_words, labels): \n",
        "  bag_of_words = bagger(msg, all_words)\n",
        "  prediction = model.predict(np.array([bag_of_words]))[0]\n",
        "  thresh = 0.2\n",
        "  y_prediction = [[idx, res] for idx, res in enumerate(prediction) if res > thresh]\n",
        "\n",
        "  y_prediction.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in y_prediction:\n",
        "    return_list.append(labels[r[0]])\n",
        "  return return_list\n",
        "\n",
        "def talk(intents_list, intents_json): \n",
        "  tag = intents_list[0]\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents: \n",
        "    if i[\"tag\"] == tag:\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return result\n",
        "\n",
        "@anvil.server.callable\n",
        "def talkLoop(msg):\n",
        "  msg = msg.lower()\n",
        "  intents_list = pred_class(msg, all_words, tags)\n",
        "  result = talk(intents_list, intents)\n",
        "  return result\n",
        "\n",
        "\n",
        "anvil.server.wait_forever()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Team 3 Chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}